{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.summarize.chain import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from helper_functions import encode_pdf, encode_from_string\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from the environment\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in your environment.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Aka Book.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retry_with_exponential_backoff(coroutine, max_retries=5):\n",
    "    \"\"\"\n",
    "    Retries a coroutine using exponential backoff upon encountering a RateLimitError.\n",
    "    \n",
    "    Args:\n",
    "        coroutine: The coroutine to be executed.\n",
    "        max_retries: The maximum number of retry attempts.\n",
    "        \n",
    "    Returns:\n",
    "        The result of the coroutine if successful.\n",
    "        \n",
    "    Raises:\n",
    "        The last encountered exception if all retry attempts fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Attempt to execute the coroutine\n",
    "            return await coroutine\n",
    "        except RateLimitError as e:\n",
    "            # If the last attempt also fails, raise the exception\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "\n",
    "            # Wait for an exponential backoff period before retrying\n",
    "            await exponential_backoff(attempt)\n",
    "\n",
    "    # If max retries are reached without success, raise an exception\n",
    "    raise Exception(\"Max retries reached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_pdf_hierarchical(path, chunk_size=1000, chunk_overlap=200, is_string=False):\n",
    "    \"\"\"\n",
    "    Asynchronously encodes a PDF book into a hierarchical vector store using OpenAI embeddings.\n",
    "    Includes rate limit handling with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing two FAISS vector stores:\n",
    "        1. Document-level summaries\n",
    "        2. Detailed chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load PDF documents\n",
    "    if not is_string:\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = await asyncio.to_thread(loader.load)\n",
    "    else:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        documents = text_splitter.create_documents([path])\n",
    "\n",
    "\n",
    "    # Create document-level summaries\n",
    "    summary_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    summary_chain = load_summarize_chain(summary_llm, chain_type=\"map_reduce\")\n",
    "    \n",
    "    async def summarize_doc(doc):\n",
    "        \"\"\"\n",
    "        Summarizes a single document with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            doc: The document to be summarized.\n",
    "            \n",
    "        Returns:\n",
    "            A summarized Document object.\n",
    "        \"\"\"\n",
    "        # Retry the summarization with exponential backoff\n",
    "        summary_output = await retry_with_exponential_backoff(summary_chain.ainvoke([doc]))\n",
    "        summary = summary_output['output_text']\n",
    "        return Document(\n",
    "            page_content=summary,\n",
    "            metadata={\"source\": path, \"page\": doc.metadata[\"page\"], \"summary\": True}\n",
    "        )\n",
    "\n",
    "    # Process documents in smaller batches to avoid rate limits\n",
    "    batch_size = 5  # Adjust this based on your rate limits\n",
    "    summaries = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_summaries = await asyncio.gather(*[summarize_doc(doc) for doc in batch])\n",
    "        summaries.extend(batch_summaries)\n",
    "        await asyncio.sleep(1)  # Short pause between batches\n",
    "\n",
    "    # Split documents into detailed chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    detailed_chunks = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
    "\n",
    "    # Update metadata for detailed chunks\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i,\n",
    "            \"summary\": False,\n",
    "            \"page\": int(chunk.metadata.get(\"page\", 0))\n",
    "        })\n",
    "\n",
    "    # Create embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Create vector stores asynchronously with rate limit handling\n",
    "    async def create_vectorstore(docs):\n",
    "        \"\"\"\n",
    "        Creates a vector store from a list of documents with rate limit handling.\n",
    "        \n",
    "        Args:\n",
    "            docs: The list of documents to be embedded.\n",
    "            \n",
    "        Returns:\n",
    "            A FAISS vector store containing the embedded documents.\n",
    "        \"\"\"\n",
    "        return await retry_with_exponential_backoff(\n",
    "            asyncio.to_thread(FAISS.from_documents, docs, embeddings)\n",
    "        )\n",
    "\n",
    "    # Generate vector stores for summaries and detailed chunks concurrently\n",
    "    summary_vectorstore, detailed_vectorstore = await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "\n",
    "    return summary_vectorstore, detailed_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muham\\anaconda3\\envs\\my_env\\lib\\site-packages\\pypdf\\generic\\_base.py:467: RuntimeWarning: coroutine 'encode_pdf_hierarchical' was never awaited\n",
      "  return float.__new__(cls, value)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_11452\\519808376.py:78: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector stores have been created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import asyncio  ### don't run this cell\n",
    "\n",
    "# Assume your PDF path is stored in the variable \"path\"\n",
    "path = \"Aka Book.pdf\"\n",
    "\n",
    "# Use asyncio.run to execute your asynchronous function\n",
    "summary_store, detailed_store = await encode_pdf_hierarchical(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(save_path)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Save the vector stores\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43msummary_store\u001b[49m\u001b[38;5;241m.\u001b[39msave_local(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_store\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     12\u001b[0m detailed_store\u001b[38;5;241m.\u001b[39msave_local(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetailed_store\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector stores have been saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summary_store' is not defined"
     ]
    }
   ],
   "source": [
    "import os ### this one saves the embeddings results, no need to run\n",
    "\n",
    "# Ensure the current working directory is correct\n",
    "save_path = os.path.join(os.getcwd(), \"vector_stores\")  # This saves in the current directory\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the vector stores\n",
    "summary_store.save_local(os.path.join(save_path, \"summary_store\"))\n",
    "detailed_store.save_local(os.path.join(save_path, \"detailed_store\"))\n",
    "\n",
    "print(f\"Vector stores have been saved in: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Performs a hierarchical retrieval using the query.\n",
    "\n",
    "    Args:\n",
    "        query: The search query.\n",
    "        summary_vectorstore: The vector store containing document summaries.\n",
    "        detailed_vectorstore: The vector store containing detailed chunks.\n",
    "        k_summaries: The number of top summaries to retrieve.\n",
    "        k_chunks: The number of detailed chunks to retrieve per summary.\n",
    "\n",
    "    Returns:\n",
    "        A list of relevant detailed chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve top summaries\n",
    "    top_summaries = summary_vectorstore.similarity_search(query, k=k_summaries)\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for summary in top_summaries:\n",
    "        # For each summary, retrieve relevant detailed chunks\n",
    "        page_number = summary.metadata[\"page\"]\n",
    "        page_filter = lambda metadata: metadata[\"page\"] == page_number\n",
    "        page_chunks = detailed_vectorstore.similarity_search(\n",
    "            query, \n",
    "            k=k_chunks, \n",
    "            filter=page_filter\n",
    "        )\n",
    "        relevant_chunks.extend(page_chunks)\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_15232\\1846038796.py:8: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "### this one reads the embeddings' results, you need to run this\n",
    "\n",
    "vector_store_path = os.path.join(os.getcwd(), \"vector_stores\")\n",
    "\n",
    "# Paths to the saved FAISS vector stores\n",
    "summary_store_path = os.path.join(vector_store_path, \"summary_store\")\n",
    "detailed_store_path = os.path.join(vector_store_path, \"detailed_store\")\n",
    "\n",
    "# Initialize OpenAI embeddings (ensure your API key is set)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load the stored vector databases\n",
    "summary_store = FAISS.load_local(summary_store_path, embeddings, allow_dangerous_deserialization=True)\n",
    "detailed_store = FAISS.load_local(detailed_store_path, embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 143\n",
      "Content: Whole milk\n",
      "Separation\n",
      "Cream\n",
      "Butter making\n",
      "Addition\n",
      "of sugar\n",
      "Addition of\n",
      "sugar and\n",
      "cocoa mass\n",
      "Buttermilk\n",
      "Cheese manufacture\n",
      "Whey\n",
      "Demineralize\n",
      "Dry Dry Dry Dry Dry\n",
      "Yoghurt\n",
      "Fermentation\n",
      "Skim milkCasein\n",
      "manufacture\n",
      "Dry Dry Dry Dry Dry\n",
      "Lactose\n",
      "Crystallize\n",
      "Demineralized\n",
      "whey powder\n",
      "Whey\n",
      "powder\n",
      "Skim milk\n",
      "(non-fat)\n",
      "powder\n",
      "Cream\n",
      "(high-fat)\n",
      "powder\n",
      "AMF/\n",
      "fractions\n",
      "Buttermilk\n",
      "powder\n",
      "Whole milk\n",
      "powder\n",
      "White\n",
      "crumb\n",
      "Chocolate\n",
      "crumb\n",
      "Yoghurt\n",
      "powder\n",
      "Ultra/f_iltration\n",
      "Figure 5.1 Flow chart of dairy processes and products used in milk chocolate....\n",
      "---\n",
      "Page: 536\n",
      "Content: Recipes   497\n",
      "The milk chocolate recipes could also be made using “chocolate crumb” to \n",
      "replace some or all of the cocoa mass, milk powder and sugar (see Chapter 6). If \n",
      "a softer milk chocolate is required, the full cream milk powder can be replaced \n",
      "with skimmed milk powder and milk fat (butter oil).\n",
      "Table 20.3 contains recipes for different types of white chocolate that can be \n",
      "made into tablets.\n",
      "When making white chocolate it is important that good quality fresh milk \n",
      "powder and high quality deodorised cocoa butter are used. It must be conched \n",
      "at 40–50 °C (104–122 °F). When a higher conching temperature is used, there is \n",
      "danger of “browning” the chocolate. If a “caramelised flavour” is preferred, the \n",
      "use of “white” chocolate crumb should be considered. When a “softer“ white \n",
      "chocolate is desired, the full cream milk powder can be replaced with skimmed \n",
      "milk powder and milk fat.\n",
      "Table 20.1 Recipes for dark tablet chocolate.\n",
      "Ingredient Range for dark \n",
      "chocolate (%)...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"describe the process differences between production of regular chocolate, milk chocolate and white chocolate?\"\n",
    "results = retrieve_hierarchical(query, summary_store, detailed_store)\n",
    "\n",
    "# Print results\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")  # Print first 100 characters\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a new class that uses your hierarchical retrieval results\n",
    "class HierarchicalRAG:\n",
    "    def __init__(self, summary_store, detailed_store):\n",
    "        # Save your stores (these are your preloaded FAISS vector stores)\n",
    "        self.summary_store = summary_store\n",
    "        self.detailed_store = detailed_store\n",
    "        \n",
    "        # Initialize the LLM\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=4000)\n",
    "        \n",
    "        # Create a prompt template similar to AdaptiveRAG\n",
    "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know—don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "        self.prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        self.llm_chain = self.prompt | self.llm\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        # Retrieve hierarchical results (this function should combine the summary and detailed stores)\n",
    "        results = retrieve_hierarchical(query, self.summary_store, self.detailed_store)\n",
    "        \n",
    "        # (Optional) Print the retrieved chunks for debugging/inspection\n",
    "        for chunk in results:\n",
    "            print(f\"Page: {chunk.metadata['page']}\")\n",
    "            print(f\"Content: {chunk.page_content[:100]}...\")  # First 100 characters\n",
    "            print(\"---\")\n",
    "        \n",
    "        # Combine all the retrieved page content into one context string.\n",
    "        # If no relevant context is found, let the LLM know by providing a fallback message.\n",
    "        if results:\n",
    "            context = \"\\n\".join([chunk.page_content for chunk in results])\n",
    "        else:\n",
    "            context = \"No relevant context found.\"\n",
    "\n",
    "        # Prepare input data for the prompt\n",
    "        input_data = {\"context\": context, \"question\": query}\n",
    "        \n",
    "        # Invoke the chain to get the answer.\n",
    "        answer = self.llm_chain.invoke(input_data)\n",
    "        return answer.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Describe the processing steps from cocoa beans to cocoa butter\"\n",
    "rag_system = HierarchicalRAG(summary_store, detailed_store)\n",
    "answer = rag_system.answer(query)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class HierarchicalRAG_mistral:\n",
    "    def __init__(self, summary_store, detailed_store):\n",
    "        self.summary_store = summary_store\n",
    "        self.detailed_store = detailed_store\n",
    "\n",
    "        # Use Ollama's locally installed Mistral model\n",
    "        self.model_name = \"mistral\"  # Ensures it uses the 7B version\n",
    "\n",
    "        # Create a prompt template similar to AdaptiveRAG\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "            If you don't know the answer, just say that you don't know—don't try to make up an answer.\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "            Answer:\"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        \"\"\"Retrieve hierarchical results and generate an answer using Mistral via Ollama.\"\"\"\n",
    "        # Retrieve hierarchical results.\n",
    "        results = retrieve_hierarchical(query, self.summary_store, self.detailed_store)\n",
    "\n",
    "        # (Optional) Debug: Print retrieved chunks.\n",
    "        for chunk in results:\n",
    "            print(f\"Page: {chunk.metadata['page']}\")\n",
    "            print(f\"Content: {chunk.page_content[:100]}...\")\n",
    "            print(\"---\")\n",
    "\n",
    "        # Combine retrieved page contents into one context string.\n",
    "        context = \"\\n\".join([chunk.page_content for chunk in results]) if results else \"No relevant context found.\"\n",
    "\n",
    "        # Prepare the input data for the prompt.\n",
    "        input_data = self.prompt.format(context=context, question=query)\n",
    "\n",
    "        # Use Ollama's Mistral model to generate a response\n",
    "        response = ollama.chat(model=self.model_name, messages=[{\"role\": \"user\", \"content\": input_data}])\n",
    "\n",
    "        # Return the generated answer\n",
    "        return response['message']['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "query = \"Describe the differences between regular chocolate, milk chocolate, and white chocolate.\"\n",
    "rag_system = HierarchicalRAG_mistral(summary_store, detailed_store)\n",
    "answer = rag_system.answer(query)\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
