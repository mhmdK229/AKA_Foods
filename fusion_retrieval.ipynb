{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from langchain.llms import Ollama \n",
    "\n",
    "load_dotenv()\n",
    "from helper_functions import replace_t_with_space, show_context\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Aka Book.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_path = os.path.join(os.getcwd(), \"vector_stores\")\n",
    "\n",
    "# Paths to the saved FAISS vector stores\n",
    "\n",
    "detailed_store_path = os.path.join(vector_store_path, \"detailed_store\")\n",
    "\n",
    "# Initialize OpenAI embeddings (ensure your API key is set)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load the stored vector databases\n",
    "detailed_store = FAISS.load_local(detailed_store_path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(documents: List[Document]) -> BM25Okapi:\n",
    "    \"\"\"\n",
    "    Create a BM25 index from the given documents.\n",
    "\n",
    "    BM25 (Best Matching 25) is a ranking function used in information retrieval.\n",
    "    It's based on the probabilistic retrieval framework and is an improvement over TF-IDF.\n",
    "\n",
    "    Args:\n",
    "    documents (List[Document]): List of documents to index.\n",
    "\n",
    "    Returns:\n",
    "    BM25Okapi: An index that can be used for BM25 scoring.\n",
    "    \"\"\"\n",
    "    # Tokenize each document by splitting on whitespace\n",
    "    # This is a simple approach and could be improved with more sophisticated tokenization\n",
    "    tokenized_docs = [doc.page_content.split() for doc in documents]\n",
    "    return BM25Okapi(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all documents from the detailed vector store\n",
    "all_docs = detailed_store.similarity_search(\"\", k=detailed_store.index.ntotal)\n",
    "bm25 = create_bm25_index(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_retrieval(vectorstore, bm25, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining keyword-based (BM25) and vector-based search.\n",
    "\n",
    "    Args:\n",
    "    vectorstore (VectorStore): The vectorstore containing the documents.\n",
    "    bm25 (BM25Okapi): Pre-computed BM25 index.\n",
    "    query (str): The query string.\n",
    "    k (int): The number of documents to retrieve.\n",
    "    alpha (float): The weight for vector search scores (1-alpha will be the weight for BM25 scores).\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: The top k documents based on the combined scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Step 1: Get all documents from the vectorstore\n",
    "    all_docs = vectorstore.similarity_search(\"\", k=vectorstore.index.ntotal)\n",
    "\n",
    "    # Step 2: Perform BM25 search\n",
    "    bm25_scores = bm25.get_scores(query.split())\n",
    "\n",
    "    # Step 3: Perform vector search\n",
    "    vector_results = vectorstore.similarity_search_with_score(query, k=len(all_docs))\n",
    "    \n",
    "    # Step 4: Normalize scores\n",
    "    vector_scores = np.array([score for _, score in vector_results])\n",
    "    vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "\n",
    "    bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) -  np.min(bm25_scores) + epsilon)\n",
    "\n",
    "    # Step 5: Combine scores\n",
    "    combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores  \n",
    "\n",
    "    # Step 6: Rank documents\n",
    "    sorted_indices = np.argsort(combined_scores)[::-1]\n",
    "    \n",
    "    # Step 7: Return top k documents\n",
    "    return [all_docs[i] for i in sorted_indices[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionRAG:\n",
    "    def __init__(self, detailed_store, bm25, k: int = 5, alpha: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initializes the FusionRAG with a detailed vector store and a BM25 index.\n",
    "        \n",
    "        Args:\n",
    "            detailed_store: A FAISS vector store containing detailed embeddings.\n",
    "            bm25: A BM25 index built from the same documents.\n",
    "            k: The number of top documents to retrieve.\n",
    "            alpha: The weight for the vector search scores in the fusion (1-alpha is the BM25 weight).\n",
    "        \"\"\"\n",
    "        self.detailed_store = detailed_store\n",
    "        self.bm25 = bm25\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Initialize the LLM (using GPT-3.5 Turbo here)\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=4000)\n",
    "\n",
    "        # Create the prompt template for answering the query based on the retrieved context.\n",
    "        prompt_template = (\n",
    "            \"Use the following pieces of context to answer the question at the end. \\n\"\n",
    "            \"If you don't know the answer, just say that you don't know—don't try to make up an answer.\\n\\n\"\n",
    "            \"{context}\\n\\n\"\n",
    "            \"Question: {question}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        self.prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        self.llm_chain = self.prompt | self.llm\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Answers a query by performing fusion retrieval on the detailed store and BM25 index,\n",
    "        then uses an LLM to generate the final answer.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The query string.\n",
    "            \n",
    "        Returns:\n",
    "            str: The LLM-generated answer.\n",
    "        \"\"\"\n",
    "        # Retrieve documents using the fusion retrieval method.\n",
    "        results = fusion_retrieval(self.detailed_store, self.bm25, query, k=self.k, alpha=self.alpha)\n",
    "        \n",
    "        # Optional: Print retrieved chunks for inspection.\n",
    "        for chunk in results:\n",
    "            page_info = chunk.metadata.get(\"page\", \"Unknown\")\n",
    "            print(f\"Page: {page_info}\")\n",
    "            print(f\"Content: {chunk.page_content[:100]}...\")  # First 100 characters for brevity\n",
    "            print(\"---\")\n",
    "        \n",
    "        # Combine retrieved context into one string; fallback if none is found.\n",
    "        if results:\n",
    "            context = \"\\n\".join([chunk.page_content for chunk in results])\n",
    "        else:\n",
    "            context = \"No relevant context found.\"\n",
    "        \n",
    "        # Prepare input data and generate the answer.\n",
    "        input_data = {\"context\": context, \"question\": query}\n",
    "        answer = self.llm_chain.invoke(input_data)\n",
    "        return answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 734\n",
      "Content: minimum level of protection is defined by the TRIPS agreement (Agreement on \n",
      "Trade‐related Aspects o...\n",
      "---\n",
      "Page: 95\n",
      "Content: 56   Chapter 3\n",
      "Ziegleder and Oberparleiter (1996) have proposed a moisture treatment prior \n",
      "to roast...\n",
      "---\n",
      "Page: 734\n",
      "Content: includes literary and artistic works. Intellectual property is an asset and can be \n",
      "bought, sold or ...\n",
      "---\n",
      "Page: 615\n",
      "Content: shows schematically the light source and the line camera. An incremental \n",
      "encoder on the fifth roll ...\n",
      "---\n",
      "Page: 554\n",
      "Content: Additionally, it is important to consider the amount of time given for rinsing \n",
      "between samples. Whe...\n",
      "---\n",
      "Final Answer:\n",
      "The optimal roasting time for cocoa is 10-15 minutes at 40-60 °C (104-140 °F) after a moisture treatment with steam.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the optimal roasting time for cocoa?\"\n",
    "fusion_rag = FusionRAG(detailed_store, bm25, k=5, alpha=0.5)\n",
    "final_answer = fusion_rag.answer(query)\n",
    "print(\"Final Answer:\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FusionRAG_Mistral:\n",
    "    def __init__(self, detailed_store, bm25, k: int = 5, alpha: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initializes the FusionRAG with a detailed vector store and a BM25 index,\n",
    "        using Ollama's Mistral model.\n",
    "\n",
    "        Args:\n",
    "            detailed_store: A FAISS vector store containing detailed embeddings.\n",
    "            bm25: A BM25 index built from the same documents.\n",
    "            k: The number of top documents to retrieve.\n",
    "            alpha: The weight for the vector search scores in the fusion (1-alpha is the BM25 weight).\n",
    "        \"\"\"\n",
    "        self.detailed_store = detailed_store\n",
    "        self.bm25 = bm25\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Initialize the LLM using Ollama's Mistral model.\n",
    "        # Note: max_tokens parameter is removed because it's not allowed.\n",
    "        self.llm = Ollama(temperature=0, model=\"mistral\")\n",
    "\n",
    "        # Create the prompt template for answering the query based on the retrieved context.\n",
    "        prompt_template = (\n",
    "            \"Use the following pieces of context to answer the question at the end. \\n\"\n",
    "            \"If you don't know the answer, just say that you don't know—don't try to make up an answer.\\n\\n\"\n",
    "            \"{context}\\n\\n\"\n",
    "            \"Question: {question}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        self.prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "        self.llm_chain = self.prompt | self.llm\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Answers a query by performing fusion retrieval on the detailed store and BM25 index,\n",
    "        then uses the LLM to generate the final answer.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string.\n",
    "\n",
    "        Returns:\n",
    "            str: The LLM-generated answer.\n",
    "        \"\"\"\n",
    "        # Retrieve documents using the fusion retrieval method.\n",
    "        results = fusion_retrieval(self.detailed_store, self.bm25, query, k=self.k, alpha=self.alpha)\n",
    "        \n",
    "        # Optional: Print retrieved chunks for inspection.\n",
    "        for chunk in results:\n",
    "            page_info = chunk.metadata.get(\"page\", \"Unknown\")\n",
    "            print(f\"Page: {page_info}\")\n",
    "            print(f\"Content: {chunk.page_content[:100]}...\")  # First 100 characters for brevity\n",
    "            print(\"---\")\n",
    "        \n",
    "        # Combine retrieved context into one string; fallback if none is found.\n",
    "        if results:\n",
    "            context = \"\\n\".join([chunk.page_content for chunk in results])\n",
    "        else:\n",
    "            context = \"No relevant context found.\"\n",
    "        \n",
    "        # Prepare input data and generate the answer.\n",
    "        input_data = {\"context\": context, \"question\": query}\n",
    "        answer = self.llm_chain.invoke(input_data)\n",
    "        return answer  # answer is returned as a string\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 734\n",
      "Content: minimum level of protection is defined by the TRIPS agreement (Agreement on \n",
      "Trade‐related Aspects o...\n",
      "---\n",
      "Page: 95\n",
      "Content: 56   Chapter 3\n",
      "Ziegleder and Oberparleiter (1996) have proposed a moisture treatment prior \n",
      "to roast...\n",
      "---\n",
      "Page: 734\n",
      "Content: includes literary and artistic works. Intellectual property is an asset and can be \n",
      "bought, sold or ...\n",
      "---\n",
      "Page: 615\n",
      "Content: shows schematically the light source and the line camera. An incremental \n",
      "encoder on the fifth roll ...\n",
      "---\n",
      "Page: 554\n",
      "Content: Additionally, it is important to consider the amount of time given for rinsing \n",
      "between samples. Whe...\n",
      "---\n",
      "Final Answer:\n",
      " The text does not provide specific information about the optimal roasting time for cocoa beans. However, it mentions that the highest roasting temperature depends upon the required roast intensity and the equipment used, and that a slow reduction in moisture content to about 3% followed by a rapid heating to the final roast temperature is the optimal way of roasting (Mohr et al., 1978). It also suggests that the processing time at 40–60 °C (104–140 °F) for the moisture treatment prior to roasting is around 10–15 minutes (Ziegleder and Oberparleiter, 1996). Without further context or additional sources, it's not possible to determine an exact optimal roasting time based on this text.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the optimal roasting time for cocoa?\"\n",
    "fusion_rag_mistral = FusionRAG_Mistral(detailed_store, bm25, k=5, alpha=0.5)\n",
    "final_answer = fusion_rag_mistral.answer(query)\n",
    "print(\"Final Answer:\")\n",
    "print(final_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
