{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_13820\\2056392876.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(item):\n",
    "    \"\"\"Extract text content from either a string or an AIMessage object.\"\"\"\n",
    "    if isinstance(item, AIMessage):\n",
    "        return item.content\n",
    "    return item\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Embed texts using OpenAIEmbeddings.\"\"\"\n",
    "    logging.info(f\"Embedding {len(texts)} texts\")\n",
    "    return embeddings.embed_documents([extract_text(text) for text in texts])\n",
    "\n",
    "def perform_clustering(embeddings: np.ndarray, n_clusters: int = 10) -> np.ndarray:\n",
    "    \"\"\"Perform clustering on embeddings using Gaussian Mixture Model.\"\"\"\n",
    "    logging.info(f\"Performing clustering with {n_clusters} clusters\")\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    return gm.fit_predict(embeddings)\n",
    "\n",
    "def summarize_texts(texts: List[str]) -> str:\n",
    "    \"\"\"Summarize a list of texts using OpenAI.\"\"\"\n",
    "    logging.info(f\"Summarizing {len(texts)} texts\")\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following text concisely:\\n\\n{text}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    input_data = {\"text\": texts}\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "def visualize_clusters(embeddings: np.ndarray, labels: np.ndarray, level: int):\n",
    "    \"\"\"Visualize clusters using PCA.\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f'Cluster Visualization - Level {level}')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_raptor_tree(texts: List[str], max_levels: int = 3) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"Build the RAPTOR tree structure with level metadata and parent-child relationships.\"\"\"\n",
    "    results = {}\n",
    "    current_texts = [extract_text(text) for text in texts]\n",
    "    current_metadata = [{\"level\": 0, \"origin\": \"original\", \"parent_id\": None} for _ in texts]\n",
    "    \n",
    "    for level in range(1, max_levels + 1):\n",
    "        logging.info(f\"Processing level {level}\")\n",
    "        \n",
    "        embeddings = embed_texts(current_texts)\n",
    "        n_clusters = min(10, len(current_texts) // 2)\n",
    "        cluster_labels = perform_clustering(np.array(embeddings), n_clusters)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'text': current_texts,\n",
    "            'embedding': embeddings,\n",
    "            'cluster': cluster_labels,\n",
    "            'metadata': current_metadata\n",
    "        })\n",
    "        \n",
    "        results[level-1] = df\n",
    "        \n",
    "        summaries = []\n",
    "        new_metadata = []\n",
    "        for cluster in df['cluster'].unique():\n",
    "            cluster_docs = df[df['cluster'] == cluster]\n",
    "            cluster_texts = cluster_docs['text'].tolist()\n",
    "            cluster_metadata = cluster_docs['metadata'].tolist()\n",
    "            summary = summarize_texts(cluster_texts)\n",
    "            summaries.append(summary)\n",
    "            new_metadata.append({\n",
    "                \"level\": level,\n",
    "                \"origin\": f\"summary_of_cluster_{cluster}_level_{level-1}\",\n",
    "                \"child_ids\": [meta.get('id') for meta in cluster_metadata],\n",
    "                \"id\": f\"summary_{level}_{cluster}\"\n",
    "            })\n",
    "        \n",
    "        current_texts = summaries\n",
    "        current_metadata = new_metadata\n",
    "        \n",
    "        if len(current_texts) <= 1:\n",
    "            results[level] = pd.DataFrame({\n",
    "                'text': current_texts,\n",
    "                'embedding': embed_texts(current_texts),\n",
    "                'cluster': [0],\n",
    "                'metadata': current_metadata\n",
    "            })\n",
    "            logging.info(f\"Stopping at level {level} as we have only one summary\")\n",
    "            break\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectore Store Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(tree_results: Dict[int, pd.DataFrame]) -> FAISS:\n",
    "    \"\"\"Build a FAISS vectorstore from all texts in the RAPTOR tree.\"\"\"\n",
    "    all_texts = []\n",
    "    all_embeddings = []\n",
    "    all_metadatas = []\n",
    "    \n",
    "    for level, df in tree_results.items():\n",
    "        all_texts.extend([str(text) for text in df['text'].tolist()])\n",
    "        all_embeddings.extend([embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in df['embedding'].tolist()])\n",
    "        all_metadatas.extend(df['metadata'].tolist())\n",
    "    \n",
    "    logging.info(f\"Building vectorstore with {len(all_texts)} texts\")\n",
    "    \n",
    "    # Create Document objects manually to ensure correct types\n",
    "    documents = [Document(page_content=str(text), metadata=metadata) \n",
    "                 for text, metadata in zip(all_texts, all_metadatas)]\n",
    "    \n",
    "    return FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tree traversal retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_traversal_retrieval(query: str, vectorstore: FAISS, k: int = 3) -> List[Document]:\n",
    "    \"\"\"Perform tree traversal retrieval.\"\"\"\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    def retrieve_level(level: int, parent_ids: List[str] = None) -> List[Document]:\n",
    "        if parent_ids:\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level and meta['id'] in parent_ids\n",
    "            )\n",
    "        else:\n",
    "            docs = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                query_embedding,\n",
    "                k=k,\n",
    "                filter=lambda meta: meta['level'] == level\n",
    "            )\n",
    "        \n",
    "        if not docs or level == 0:\n",
    "            return docs\n",
    "        \n",
    "        child_ids = [doc.metadata.get('child_ids', []) for doc, _ in docs]\n",
    "        child_ids = [item for sublist in child_ids for item in sublist]  # Flatten the list\n",
    "        \n",
    "        child_docs = retrieve_level(level - 1, child_ids)\n",
    "        return docs + child_docs\n",
    "    \n",
    "    max_level = max(doc.metadata['level'] for doc in vectorstore.docstore.values())\n",
    "    return retrieve_level(max_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vectorstore: FAISS) -> ContextualCompressionRetriever:\n",
    "    \"\"\"Create a retriever with contextual compression.\"\"\"\n",
    "    logging.info(\"Creating contextual compression retriever\")\n",
    "    base_retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context and question, extract only the relevant information for answering the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Relevant Information:\"\n",
    "    )\n",
    "    \n",
    "    extractor = LLMChainExtractor.from_llm(llm, prompt=prompt)\n",
    "    \n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=extractor,\n",
    "        base_retriever=base_retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hierarchical retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_retrieval(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> List[Document]:\n",
    "    \"\"\"Perform hierarchical retrieval starting from the highest level, handling potential None values.\"\"\"\n",
    "    all_retrieved_docs = []\n",
    "    \n",
    "    for level in range(max_level, -1, -1):\n",
    "        # Retrieve documents from the current level\n",
    "        level_docs = retriever.get_relevant_documents(\n",
    "            query,\n",
    "            filter=lambda meta: meta['level'] == level\n",
    "        )\n",
    "        all_retrieved_docs.extend(level_docs)\n",
    "        \n",
    "        # If we've found documents, retrieve their children from the next level down\n",
    "        if level_docs and level > 0:\n",
    "            child_ids = [doc.metadata.get('child_ids', []) for doc in level_docs]\n",
    "            child_ids = [item for sublist in child_ids for item in sublist if item is not None]  # Flatten and filter None\n",
    "            \n",
    "            if child_ids:  # Only modify query if there are valid child IDs\n",
    "                child_query = f\" AND id:({' OR '.join(str(id) for id in child_ids)})\"\n",
    "                query += child_query\n",
    "    \n",
    "    return all_retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raptor_query(query: str, retriever: ContextualCompressionRetriever, max_level: int) -> Dict[str, Any]:\n",
    "    \"\"\"Process a query using the RAPTOR system with hierarchical retrieval.\"\"\"\n",
    "    logging.info(f\"Processing query: {query}\")\n",
    "    \n",
    "    relevant_docs = hierarchical_retrieval(query, retriever, max_level)\n",
    "    \n",
    "    doc_details = []\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        doc_details.append({\n",
    "            \"index\": i,\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "            \"level\": doc.metadata.get('level', 'Unknown'),\n",
    "            \"similarity_score\": doc.metadata.get('score', 'N/A')\n",
    "        })\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Given the following context, please answer the question:\\n\\n\"\n",
    "        \"Context: {context}\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    answer = chain.run(context=context, question=query)\n",
    "    \n",
    "    logging.info(\"Query processing completed\")\n",
    "    \n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": doc_details,\n",
    "        \"num_docs_retrieved\": len(relevant_docs),\n",
    "        \"context_used\": context,\n",
    "        \"answer\": answer,\n",
    "        \"model_used\": llm.model_name,\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def print_query_details(result: Dict[str, Any]):\n",
    "    \"\"\"Print detailed information about the query process, including tree level metadata.\"\"\"\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"\\nNumber of documents retrieved: {result['num_docs_retrieved']}\")\n",
    "    print(f\"\\nRetrieved Documents:\")\n",
    "    for doc in result['retrieved_documents']:\n",
    "        print(f\"  Document {doc['index']}:\")\n",
    "        print(f\"    Content: {doc['content'][:100]}...\")  # Show first 100 characters\n",
    "        print(f\"    Similarity Score: {doc['similarity_score']}\")\n",
    "        print(f\"    Tree Level: {doc['metadata'].get('level', 'Unknown')}\")\n",
    "        print(f\"    Origin: {doc['metadata'].get('origin', 'Unknown')}\")\n",
    "        if 'child_docs' in doc['metadata']:\n",
    "            print(f\"    Number of Child Documents: {len(doc['metadata']['child_docs'])}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nContext used for answer generation:\")\n",
    "    print(result['context_used'])\n",
    "    \n",
    "    print(f\"\\nGenerated Answer:\")\n",
    "    print(result['answer'])\n",
    "    \n",
    "    print(f\"\\nModel Used: {result['model_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"Aka Book.pdf\"\n",
    "loader = PyPDFLoader(path)\n",
    "documents = loader.load()\n",
    "texts = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 03:21:16,926 - INFO - Processing level 1\n",
      "2025-02-11 03:21:16,930 - INFO - Embedding 801 texts\n",
      "2025-02-11 03:21:26,920 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:21:28,842 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:21:29,370 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:21:32,177 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:21:32,422 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:21:32,556 - INFO - Performing clustering with 10 clusters\n",
      "2025-02-11 03:21:45,748 - INFO - Summarizing 97 texts\n",
      "2025-02-11 03:21:51,737 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:21:51,766 - INFO - Summarizing 84 texts\n",
      "2025-02-11 03:22:03,934 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:22:03,939 - INFO - Summarizing 30 texts\n",
      "2025-02-11 03:22:15,399 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:22:15,401 - INFO - Summarizing 137 texts\n",
      "2025-02-11 03:22:28,439 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:22:28,453 - INFO - Summarizing 102 texts\n",
      "2025-02-11 03:22:49,061 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:22:49,073 - INFO - Summarizing 133 texts\n",
      "2025-02-11 03:23:08,803 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:23:08,828 - INFO - Summarizing 107 texts\n",
      "2025-02-11 03:23:24,409 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:23:24,416 - INFO - Summarizing 41 texts\n",
      "2025-02-11 03:23:27,507 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:23:27,519 - INFO - Summarizing 34 texts\n",
      "2025-02-11 03:23:41,914 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:23:42,045 - INFO - Summarizing 36 texts\n",
      "2025-02-11 03:23:46,730 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:23:46,739 - INFO - Processing level 2\n",
      "2025-02-11 03:23:46,739 - INFO - Embedding 10 texts\n",
      "2025-02-11 03:23:47,734 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:23:48,060 - INFO - Performing clustering with 5 clusters\n",
      "2025-02-11 03:23:59,889 - INFO - Summarizing 2 texts\n",
      "2025-02-11 03:24:02,962 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:24:02,968 - INFO - Summarizing 5 texts\n",
      "2025-02-11 03:24:05,061 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:24:05,071 - INFO - Summarizing 1 texts\n",
      "2025-02-11 03:24:06,828 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:24:06,836 - INFO - Summarizing 1 texts\n",
      "2025-02-11 03:24:08,855 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:24:08,864 - INFO - Summarizing 1 texts\n",
      "2025-02-11 03:24:13,913 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:24:14,012 - INFO - Processing level 3\n",
      "2025-02-11 03:24:14,012 - INFO - Embedding 5 texts\n",
      "2025-02-11 03:24:14,453 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:24:14,544 - INFO - Performing clustering with 2 clusters\n",
      "2025-02-11 03:24:17,547 - INFO - Summarizing 4 texts\n",
      "2025-02-11 03:24:22,475 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:24:22,479 - INFO - Summarizing 1 texts\n",
      "2025-02-11 03:24:24,157 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "tree_results = build_raptor_tree(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 03:25:13,151 - INFO - Building vectorstore with 816 texts\n",
      "2025-02-11 03:25:20,665 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:25:22,156 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:25:22,435 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:25:23,099 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:25:23,348 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:25:23,611 - INFO - Loading faiss with AVX2 support.\n",
      "2025-02-11 03:25:23,791 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-02-11 03:25:23,825 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = build_vectorstore(tree_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 03:25:35,259 - INFO - Creating contextual compression retriever\n"
     ]
    }
   ],
   "source": [
    "retriever = create_retriever(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 03:31:01,012 - INFO - Processing query: what can I replace cocoa butter with?\n",
      "2025-02-11 03:31:02,097 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:02,391 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:02,696 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:03,191 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:10,319 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:12,296 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:15,211 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:16,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:18,214 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-11 03:31:18,231 - INFO - Query processing completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what can I replace cocoa butter with?\n",
      "\n",
      "Number of documents retrieved: 4\n",
      "\n",
      "Retrieved Documents:\n",
      "  Document 1:\n",
      "    Content: You can replace cocoa butter with CBE (cocoa butter equivalent) fats or cocoa butter substitute (CBS...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 2:\n",
      "    Content: Cocoa butter can be replaced with cocoa butter substitutes (CBSs), which are fully refined fats prod...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 3:\n",
      "    Content: You can replace cocoa butter with non-lauric cocoa butter replacers (CBRs) produced from non-lauric ...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "  Document 4:\n",
      "    Content: Cocoa butter equivalents (CBEs) are fats that have similar fatty acid and triglyceride compositions ...\n",
      "    Similarity Score: N/A\n",
      "    Tree Level: 0\n",
      "    Origin: original\n",
      "\n",
      "\n",
      "Context used for answer generation:\n",
      "You can replace cocoa butter with CBE (cocoa butter equivalent) fats or cocoa butter substitute (CBS) coatings made with lauric fats, such as HPKO and coconut oil.\n",
      "\n",
      "Cocoa butter can be replaced with cocoa butter substitutes (CBSs), which are fully refined fats produced from palm kernel and/or coconut oil. These substitutes have characteristics similar to cocoa butter in terms of hardness, mouthfeel, and flavour release, but they contain a high level of lauric fatty acids and have a different triglyceride composition. However, it is important to note that the maximum safe addition of cocoa butter to lauric CBSs is about 5%, which limits the cocoa inclusion in recipes based on lauric CBSs to low-fat or fat-free cocoa powder rather than cocoa mass.\n",
      "\n",
      "You can replace cocoa butter with non-lauric cocoa butter replacers (CBRs) produced from non-lauric oils such as palm oil and soybean oil. These replacers can include hydrogenated and fractionated types, though they have limited compatibility with cocoa butter. You can add up to 7% of non-lauric CBR to cocoa butter or allow up to approximately 25% cocoa butter in the fat phase of non-lauric fat-based coatings.\n",
      "\n",
      "Cocoa butter equivalents (CBEs) are fats that have similar fatty acid and triglyceride compositions to cocoa butter, allowing them to have similar physical characteristics. Some vegetable oils contain triglycerides of the appropriate structure, making it possible to produce specific fat fractions that can be blended to have properties almost identical to those of cocoa butter. Major manufacturers in the UK agreed to use a maximum of 5% CBE on the total weight of the chocolate.\n",
      "\n",
      "Generated Answer:\n",
      "You can replace cocoa butter with cocoa butter equivalents (CBEs), cocoa butter substitutes (CBS) made from lauric fats (such as HPKO and coconut oil), or non-lauric cocoa butter replacers (CBRs) produced from non-lauric oils (like palm oil and soybean oil).\n",
      "\n",
      "Model Used: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "max_level = 3  # Adjust based on your tree depth\n",
    "query = \"what can I replace cocoa butter with?\"\n",
    "result = raptor_query(query, retriever, max_level)\n",
    "print_query_details(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_results type: <class 'dict'>\n",
      "vectorstore type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "retriever type: <class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'>\n"
     ]
    }
   ],
   "source": [
    "print(\"tree_results type:\", type(tree_results))\n",
    "print(\"vectorstore type:\", type(vectorstore))\n",
    "print(\"retriever type:\", type(retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_results saved at C:\\Users\\muham\\OneDrive\\Desktop\\aka foods project\\raptor_tree_results.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the save directory\n",
    "save_dir = r\"C:\\Users\\muham\\OneDrive\\Desktop\\aka foods project\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "tree_results_path = os.path.join(save_dir, \"raptor_tree_results.pkl\")\n",
    "\n",
    "# Save tree_results\n",
    "with open(tree_results_path, \"wb\") as f:\n",
    "    pickle.dump(tree_results, f)\n",
    "\n",
    "print(f\"tree_results saved at {tree_results_path}\")\n",
    "\n",
    "\n",
    "faiss_path = os.path.join(save_dir, \"raptor_faiss_index\")\n",
    "\n",
    "# Save FAISS vectorstore\n",
    "vectorstore.save_local(faiss_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
